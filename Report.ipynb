{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# INF264 Project 1:\n",
    "## Implementing decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Introduction\n",
    "This task has been a collaboration between Marius L. Hatland and Sigurd Blakkestad. The division of labor has been about Marius 70% and Sigurd 30%\n",
    "\n",
    "In this report we will explain how we have implemented a decision tree learning algorithm from scratch. We will discuss how we approached the task and which design choices were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.1-1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implement a decision tree learning algorithm from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first design choice we made was to use the Pandas library for most calculations and data manipulation. We decided this because we are more experienced in using Pandas. While numpy probably would make our algorithm faster, we felt this was an acceptable tradeoff as there was no points to gain from a faster algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To begin implementing the greedy algorithm we first need to implemented the basic data structures needed to build a tree. We have implemented a Node class which will either have a Data class (A class containing information about the split value, split index and majority label) and children nodes or a label if it is a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, split_value, split_index, majority_label):\n",
    "        self.split_value = split_value\n",
    "        self.split_index = split_index\n",
    "        self.majority_label = majority_label\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label=None, data=None):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we created a decision tree class which instantiates a root node in its constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = Node()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From there we created a function which builds the tree as described in the project description as seen in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_tree(X, y, impurity_measure, node):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    #If all labels in y are equal: assign that label to the node\n",
    "    unique_labels_in_y = set(y)\n",
    "    if len(unique_labels_in_y) == 1:\n",
    "        node.label = y.iloc[0]\n",
    "        return\n",
    "    #If all feature values in X are identical: assign the majority label to the node\n",
    "    elif has_identical_feature_values(X):\n",
    "        node.label = get_majority_label(df)\n",
    "        return\n",
    "    else:\n",
    "        #Find out wich feature gives the highest information gain\n",
    "        split_info = get_feature_with_highest_information_gain(df, impurity_measure)\n",
    "\n",
    "        #Assign the optimal split value and split index to the current node | Also set the majority label as we need this later for pruning\n",
    "        node.data = Data(split_info['split_value'], split_info['split_index'], get_majority_label(df))\n",
    "        node.left = Node()\n",
    "        node.right = Node()\n",
    "\n",
    "        X_below = split_info['below_split'].iloc[:, :-1]\n",
    "        y_below = split_info['below_split'].iloc[:, -1]\n",
    "\n",
    "        X_above = split_info['above_split'].iloc[:, :-1]\n",
    "        y_above = split_info['above_split'].iloc[:, -1]\n",
    "\n",
    "        #Recursively continue to the left and right\n",
    "        create_tree(X_below, y_below, impurity_measure, node.left)\n",
    "        create_tree(X_above, y_above, impurity_measure, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Impurity and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We decided to calculate both entropy and the gini index in a single function to get the impurity. Then  we calculated the information gain from each feature and chose the feature with the highest gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_impurity(data, impurity_measure):\n",
    "    y = data.iloc[:, -1]\n",
    "    _, labels = np.unique(y, return_counts=True)\n",
    "    prob_current_label = labels / np.sum(labels)\n",
    "\n",
    "    if impurity_measure == 'entropy':\n",
    "        return -1 * np.sum(prob_current_label * np.log2(prob_current_label))\n",
    "\n",
    "    if impurity_measure == 'gini':\n",
    "        return 1 - np.sum(prob_current_label ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When building the decision tree we made sure to store the majority label in each node, so we didn't have to calculate it in the pruning process. The pruning is implemented in a recursive manner, where we start from the leaves and work our way upwards to the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the code we trained 4 models (entropy or gini, with pruning or without pruning) and we have decided to use accuracy_score as our metric to evaluate the performance. We then chose the model that got the highest accuracy on unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree entropy \n",
      "Training accuracy:  1.0\n",
      "Validation accuracy:  0.7957413249211357\n",
      "\n",
      "Decision tree entropy Pruning\n",
      "Training accuracy:  0.8807395723799509\n",
      "Validation accuracy:  0.8283385909568874\n",
      "\n",
      "Decision tree gini \n",
      "Training accuracy:  1.0\n",
      "Validation accuracy:  0.7915352260778128\n",
      "\n",
      "Decision tree gini Pruning\n",
      "Training accuracy:  0.8803890641430073\n",
      "Validation accuracy:  0.830441640378549\n",
      "\n",
      "The best model based on the validation data is:\n",
      "Decision tree using impurity measure=gini and with pruning\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model selection\n",
    "val_accuracies = []\n",
    "for model in models:\n",
    "    print('Decision tree', model.impurity_measure, \"Pruning\" if model.pruning else '')\n",
    "    print(\"Training accuracy: \", accuracy_score(y_train, model.predict(X_train)))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(\"Validation accuracy: \", val_acc)\n",
    "    print()\n",
    "\n",
    "#Select the best model based on highes validation accuracy\n",
    "best_model = models[list.index(val_accuracies, max(val_accuracies))]\n",
    "\n",
    "print(\"The best model based on the validation data is:\")\n",
    "print(best_model)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your estimate for the performance of\n",
    "the selected model on unseen data points? Report how you arrived at the\n",
    "conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation\n",
      "Best model's accuracy on test data:  0.8438485804416404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation\n",
    "print(\"Model evaluation\")\n",
    "test_pred = best_model.predict(X_test)\n",
    "print(\"Best model's accuracy on test data: \", accuracy_score(y_test, test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn's descision tree classiffier: \n",
      "Validation accuracy:  0.8154574132492114\n",
      "Test accuracy:  0.8172975814931651\n"
     ]
    }
   ],
   "source": [
    "#Comparing to existing implementation\n",
    "sk_learn_decision_tree = DecisionTreeClassifier(random_state=42, criterion=str(best_model.impurity_measure))\n",
    "sk_learn_decision_tree.fit(X_train, y_train)\n",
    "val_pred = sk_learn_decision_tree.predict(X_val)\n",
    "test_pred = sk_learn_decision_tree.predict(X_test)\n",
    "\n",
    "print(\"Sklearn's descision tree classiffier: \")\n",
    "print(\"Validation accuracy: \", accuracy_score(y_val, sk_learn_decision_tree.predict(X_val)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, sk_learn_decision_tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = Node()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Decision tree using impurity measure=\" + str(self.impurity_measure) + ' and with pruning' if self.pruning else ' without pruning'\n",
    "\n",
    "    \"\"\"\n",
    "    Fits the decision tree to data provided\n",
    "    Arguments:\n",
    "        X: A pandas dataframe containing all features in a dataset\n",
    "        y: A pandas series containing all labels in a dataset\n",
    "        impurity_measure: A String. Can either be entropy or gini\n",
    "        pruning: Boolean. If true: reduced error pruning will be performed\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def learn(self, X, y, impurity_measure='entropy', pruning=False):\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.pruning = pruning\n",
    "\n",
    "        if self.pruning:\n",
    "            #Split into training and pruning data\n",
    "            X_train, X_prune, y_train, y_prune = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "            #Fit the decision tree to the training data\n",
    "            create_tree(X_train, y_train, impurity_measure, self.tree)\n",
    "            #Perform reduced error pruning\n",
    "            prune(X_prune, y_prune, self.tree)\n",
    "\n",
    "        else:\n",
    "            #Fit the decision tree to all data given to learn()\n",
    "            create_tree(X, y, impurity_measure, self.tree)\n",
    "\n",
    "    \"\"\"\n",
    "    Predicts label for all rows x in a matrix X\n",
    "    Arguments:\n",
    "        X: A pandas dataframe containing all features in a dataset\n",
    "    Returns:\n",
    "        List containing all the predicted labels\n",
    "    \"\"\"\n",
    "    def predict(self, X):\n",
    "        #Stores all predictions\n",
    "        predictions = []\n",
    "        #For each row in x; traverse the tree to get the predicted label and append it to the list\n",
    "        for i in range(len(X)):\n",
    "            x = X.iloc[i, :]\n",
    "            predictions.append(get_prediction_label(x, self.tree))\n",
    "        return predictions\n",
    "\n",
    "\"\"\"\n",
    "Predicts label for a given row x\n",
    "Arguments:\n",
    "    x: A pandas series containing a row in a dataset\n",
    "    node: a Node (the current node). This node is used to perform the method recursively\n",
    "Returns:\n",
    "    String - Predicted label\n",
    "\"\"\"\n",
    "def get_prediction_label(x, node):\n",
    "    # Return label when a leaf is reached\n",
    "    if node.is_leaf():\n",
    "        return node.label\n",
    "    # Otherwise continue traversing the tree (recursively)\n",
    "    elif x[node.data.split_index] < node.data.split_value:\n",
    "        return get_prediction_label(x, node.left)\n",
    "    else:\n",
    "        return get_prediction_label(x, node.right)\n",
    "\n",
    "\"\"\"\n",
    "Function to perform reduced error pruning\n",
    "Arguments:\n",
    "    X: A pandas dataframe containing the features in a dataset\n",
    "    y: A pandas series containing the labels in a dataset\n",
    "    node: a Node (the current node). This node is used to perform the method recursively\n",
    "Returns:\n",
    "    int - Label error. This result is used in the function callbacks\n",
    "\"\"\"\n",
    "def prune(X, y, node):\n",
    "    if node.is_leaf():\n",
    "        #Returns amount of label errors\n",
    "        return len(y) - y.tolist().count(node.label)\n",
    "\n",
    "    # If no datapoints below/above split return 0 label errors\n",
    "    if X.empty:\n",
    "        return 0\n",
    "\n",
    "    dataset = pd.concat([X, y], axis=1)\n",
    "\n",
    "    #Split the dataset\n",
    "    above_split, below_split = split_dataset(dataset, node.data.split_index, node.data.split_value)\n",
    "\n",
    "    #Extract X and y in above and below\n",
    "    X_below = below_split.iloc[:, :-1]\n",
    "    y_below = below_split.iloc[:, -1]\n",
    "    X_above = above_split.iloc[:, :-1]\n",
    "    y_above = above_split.iloc[:, -1]\n",
    "\n",
    "\n",
    "    label_errors_left_node = prune(X_below, y_below, node.left)\n",
    "    label_errors_right_node = prune(X_above, y_above, node.right)\n",
    "    label_errors_majority_label = len(y) - y.tolist().count(node.data.majority_label)\n",
    "\n",
    "    #Cut off subtree if we get fewer or the same amount of errors using the majority label\n",
    "    if label_errors_majority_label <= label_errors_left_node + label_errors_right_node:\n",
    "        node.label = node.data.majority_label\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "        return label_errors_majority_label\n",
    "    return label_errors_left_node + label_errors_right_node\n",
    "\n",
    "'''\n",
    "Fits the decision tree to data provided\n",
    "Arguments:\n",
    "    X: A pandas dataframe containing all features in a dataset\n",
    "    y: A pandas series containing all labels in a dataset\n",
    "    impurity_measure: A String. Can either be entropy or gini\n",
    "    node: a Node (the current node). This node is used to perform the method recursively\n",
    "Returns:\n",
    "    Nothing\n",
    "'''\n",
    "def create_tree(X, y, impurity_measure, node):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    #If all labels in y are equal: assign that label to the node\n",
    "    unique_labels_in_y = set(y)\n",
    "    if len(unique_labels_in_y) == 1:\n",
    "        node.label = y.iloc[0]\n",
    "        return\n",
    "    #If all feature values in X are identical: assign the majority label to the node\n",
    "    elif has_identical_feature_values(X):\n",
    "        node.label = get_majority_label(df)\n",
    "        return\n",
    "    else:\n",
    "        #Find out wich feature gives the highest information gain\n",
    "        split_info = get_feature_with_highest_information_gain(df, impurity_measure)\n",
    "\n",
    "        #Assign the optimal split value and split index to the current node | Also set the majority label as we need this later for pruning\n",
    "        node.data = Data(split_info['split_value'], split_info['split_index'], get_majority_label(df))\n",
    "        node.left = Node()\n",
    "        node.right = Node()\n",
    "\n",
    "        X_below = split_info['below_split'].iloc[:, :-1]\n",
    "        y_below = split_info['below_split'].iloc[:, -1]\n",
    "\n",
    "        X_above = split_info['above_split'].iloc[:, :-1]\n",
    "        y_above = split_info['above_split'].iloc[:, -1]\n",
    "\n",
    "        #Recursively continue to the left and right\n",
    "        create_tree(X_below, y_below, impurity_measure, node.left)\n",
    "        create_tree(X_above, y_above, impurity_measure, node.right)\n",
    "\n",
    "'''\n",
    "Calculates impurity\n",
    "Arguments:\n",
    "    data: a pandas dataframe containing both features and labels\n",
    "    impurity_measure: A String. Can either be entropy or gini\n",
    "Returns:\n",
    "    float: impurity\n",
    "'''\n",
    "def calculate_impurity(data, impurity_measure):\n",
    "    y = data.iloc[:, -1]\n",
    "    _, labels = np.unique(y, return_counts=True)\n",
    "    prob_current_label = labels / np.sum(labels)\n",
    "\n",
    "    if impurity_measure == 'entropy':\n",
    "        return -1 * np.sum(prob_current_label * np.log2(prob_current_label))\n",
    "\n",
    "    if impurity_measure == 'gini':\n",
    "        return 1 - np.sum(prob_current_label ** 2)\n",
    "\n",
    "'''\n",
    "Splits a dataset based on the value of a given index and a given value\n",
    "Argements:\n",
    "    data: a pandas dataframe containing both features and labels\n",
    "    column_index: int - The index of the column index to split on\n",
    "    split_value: float - The value to split on\n",
    "Returns:\n",
    "    above_split: a pandas dataframe containing the rows above the split\n",
    "    below_split: a pandas dataframe containing the rows below the split\n",
    "'''\n",
    "def split_dataset(data, column_index, split_value):\n",
    "    above_split = data.loc[data.iloc[:, column_index] >= split_value]\n",
    "    below_split = data.loc[data.iloc[:, column_index] < split_value]\n",
    "    return above_split, below_split\n",
    "\n",
    "'''\n",
    "Calculates the information gain from a feature\n",
    "Arguments:\n",
    "    data: a pandas dataframe containing both features and labels\n",
    "    column_index: int - The index of the column index to split on\n",
    "    split: float - The value to split on\n",
    "    impurity_measure: A String. Can either be entropy or gini\n",
    "Returns:\n",
    "    split_info: A dictionary -\n",
    "        \"information_gain\": information_gain, float\n",
    "        \"split_value\": split_value, float\n",
    "        \"split_index\": column_index, int\n",
    "        \"above_split\": above_split, pandas dataframe\n",
    "        \"below_split\": below_split pandas dataframe\n",
    "'''\n",
    "def calculate_information_gain_of_feature(data, column_index, split, impurity_measure):\n",
    "    split_value = 0\n",
    "    if split == 'mean':\n",
    "        split_value = data.iloc[:, column_index].mean()\n",
    "    elif split == 'median':\n",
    "        split_value = data.iloc[:, column_index].median()\n",
    "    else:\n",
    "        raise Exception('Split mode not recognized')\n",
    "\n",
    "    above_split, below_split = split_dataset(data, column_index, split_value)\n",
    "\n",
    "    impurity_above_split = calculate_impurity(above_split, impurity_measure=impurity_measure)\n",
    "    impurity_below_split = calculate_impurity(below_split, impurity_measure=impurity_measure)\n",
    "\n",
    "    information = len(above_split) / len(data) * impurity_above_split + len(below_split) / len(\n",
    "        data) * impurity_below_split\n",
    "\n",
    "    information_gain = calculate_impurity(data, impurity_measure=impurity_measure) - information\n",
    "\n",
    "    split_info = {\n",
    "        \"information_gain\": information_gain,\n",
    "        \"split_value\": split_value,\n",
    "        \"split_index\": column_index,\n",
    "        \"above_split\": above_split,\n",
    "        \"below_split\": below_split\n",
    "    }\n",
    "\n",
    "    return split_info\n",
    "\n",
    "'''\n",
    "Finds which feature gives the highest information gain as well as the features index and split value\n",
    "Arguments:\n",
    "    data: a pandas dataframe containing both features and labels\n",
    "    impurity_measure: A String. Can either be entropy or gini\n",
    "Returns:\n",
    "    features_with_highest_information_gain: A dictionary -\n",
    "        \"information_gain\": information_gain, float\n",
    "        \"split_value\": split_value, float\n",
    "        \"split_index\": column_index, int\n",
    "        \"above_split\": above_split, pandas dataframe\n",
    "        \"below_split\": below_split pandas dataframe\n",
    "'''\n",
    "def get_feature_with_highest_information_gain(data, impurity_measure, split='mean'):\n",
    "    information_gains = []\n",
    "    for i in range(data.shape[1] - 1):\n",
    "        information_gains.append(\n",
    "            calculate_information_gain_of_feature(data, i, split=split, impurity_measure=impurity_measure))\n",
    "\n",
    "    feauture_with_highest_information_gain = information_gains[0]\n",
    "    for i in range(1, len(information_gains)):\n",
    "        if information_gains[i][\"information_gain\"] > feauture_with_highest_information_gain[\"information_gain\"]:\n",
    "            feauture_with_highest_information_gain = information_gains[i]\n",
    "\n",
    "    return feauture_with_highest_information_gain\n",
    "\n",
    "'''\n",
    "Checks if all the rows in a dataframe are equal\n",
    "Arguments:\n",
    "    X: a pandas dataframe containing the features of a dataset\n",
    "Returns:\n",
    "    Boolean: True if all rows in the dataframe are equal; False otherwise\n",
    "'''\n",
    "def has_identical_feature_values(X):\n",
    "    # Finds firs row\n",
    "    first = X.iloc[0, :]\n",
    "\n",
    "    # Creates a new boolean dataframe based on which rows in X are equal to the first row\n",
    "    df = X == first\n",
    "\n",
    "    # Returns true if all values in df are true; False otherwise\n",
    "    return df.all().all()\n",
    "\n",
    "'''\n",
    "Finds the majority label in a dataset\n",
    "Arguments:\n",
    "    data: a pandas dataframe\n",
    "Returns:\n",
    "    String: the majority label\n",
    "'''\n",
    "def get_majority_label(data):\n",
    "    # Get counts for each label\n",
    "    value_counts = data.iloc[:, -1].value_counts()\n",
    "\n",
    "    # Sort in descending order and return the largest count\n",
    "    return value_counts.sort_values(ascending=False).keys()[0]\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, split_value, split_index, majority_label):\n",
    "        self.split_value = split_value\n",
    "        self.split_index = split_index\n",
    "        self.majority_label = majority_label\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label=None, data=None):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    '''\n",
    "    Checks if this node is a leaf\n",
    "    Returns:\n",
    "        Boolean: True if node is leaf; False otherwise\n",
    "    '''\n",
    "    def is_leaf(self):\n",
    "        if self.left is None and self.right is None:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #Used for debugging\n",
    "    def __str__(self):\n",
    "        if self.is_leaf():\n",
    "            return \"Leaf node with label \" + str(self.label)\n",
    "        else:\n",
    "            return 'Split index ' + str(self.data.split_index) + '\\nSplit value ' + str(\n",
    "                self.data.split_value) + '\\nMajority label ' + str(self.data.majority_label)\n",
    "\n",
    "\n",
    "# Reading the data\n",
    "data = pd.read_csv('magic04.data', header=None)\n",
    "\n",
    "# Splitting data into X and y\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Splitting data into X_train, y_train, X_val, y_val, X_test, y_test\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Training\n",
    "models = []\n",
    "\n",
    "decision_tree_entropy = DecisionTree()\n",
    "decision_tree_entropy.learn(X_train, y_train)\n",
    "models.append(decision_tree_entropy)\n",
    "\n",
    "decision_tree_entropy_pruning = DecisionTree()\n",
    "decision_tree_entropy_pruning.learn(X_train, y_train, pruning=True)\n",
    "models.append(decision_tree_entropy_pruning)\n",
    "\n",
    "decision_tree_gini = DecisionTree()\n",
    "decision_tree_gini.learn(X_train, y_train, impurity_measure='gini')\n",
    "models.append(decision_tree_gini)\n",
    "\n",
    "decision_tree_gini_pruning = DecisionTree()\n",
    "decision_tree_gini_pruning.learn(X_train, y_train, impurity_measure='gini', pruning=True)\n",
    "models.append(decision_tree_gini_pruning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
